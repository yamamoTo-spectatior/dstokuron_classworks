{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea90604e-1eee-485a-a17e-dfe32e433b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº† ---\n",
      "\n",
      "--- 1_analysis_clusters.csv ã®åˆ—å ---\n",
      "Index(['artist', 'total_tv_appearances', 'spotify_popularity',\n",
      "       'youtube_hit_appearances', 'google_trends_score', 'cluster'],\n",
      "      dtype='object')\n",
      "\n",
      "--- artist_data.csv (ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿) ã®æœ€å¤§å¹´: 2024 ---\n",
      "info: 'cluster' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚'cluster_label'ã¨ã—ã¦ç‰¹å¾´é‡ã«çµåˆã—ã¾ã™ã€‚\n",
      "\n",
      "--- ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ï¼ˆyear, artistï¼‰ã‚’å‰Šé™¤ä¸­ ---\n",
      "åˆæœŸè¡Œæ•°: 7498, é‡è¤‡å‰Šé™¤å¾Œè¡Œæ•°: 947\n",
      "\n",
      "--- ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆå®Œäº† ---\n",
      "is_kohaku_performer ã®åˆ†å¸ƒ:\n",
      "is_kohaku_performer\n",
      "0    546\n",
      "1    401\n",
      "Name: count, dtype: int64\n",
      "\n",
      "çµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­:\n",
      "    year     artist  total_tv_appearances_yearly  appearances_NHKç´…ç™½æ­Œåˆæˆ¦  \\\n",
      "0   2015        AAA                          6.0                   6.0   \n",
      "3   2015         AI                          0.0                   0.0   \n",
      "15  2015      AKB48                          8.0                   8.0   \n",
      "34  2015  BABYMETAL                          0.0                   0.0   \n",
      "38  2015   BREAKERZ                          0.0                   0.0   \n",
      "\n",
      "    appearances_ãƒ™ã‚¹ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ  appearances_ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ©ã‚¤ãƒ– artist_category  \\\n",
      "0                     0.0                              0.0      ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆæ€§åˆ¥ä¸æ˜ï¼‰   \n",
      "3                     0.0                              0.0            å¥³æ€§ã‚½ãƒ­   \n",
      "15                    0.0                              0.0          å¥³æ€§ã‚°ãƒ«ãƒ¼ãƒ—   \n",
      "34                    0.0                              0.0          å¥³æ€§ã‚°ãƒ«ãƒ¼ãƒ—   \n",
      "38                    0.0                              0.0      ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆæ€§åˆ¥ä¸æ˜ï¼‰   \n",
      "\n",
      "        type       origin  spotify_followers  spotify_popularity  \\\n",
      "0      Group        Japan              756.0                16.0   \n",
      "3   femaleã‚½ãƒ­  Los Angeles            31319.0                54.0   \n",
      "15    å¥³æ€§ã‚°ãƒ«ãƒ¼ãƒ—           æ—¥æœ¬           511629.0                59.0   \n",
      "34    å¥³æ€§ã‚°ãƒ«ãƒ¼ãƒ—           æ—¥æœ¬          2597237.0                71.0   \n",
      "38     Group           æ—¥æœ¬            13246.0                33.0   \n",
      "\n",
      "    cluster_label  is_kohaku_performer  \n",
      "0               1                    1  \n",
      "3               2                    0  \n",
      "15              2                    1  \n",
      "34              0                    0  \n",
      "38              0                    0  \n",
      "\n",
      "çµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æƒ…å ±:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 947 entries, 0 to 7494\n",
      "Data columns (total 13 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   year                             947 non-null    int64  \n",
      " 1   artist                           947 non-null    object \n",
      " 2   total_tv_appearances_yearly      947 non-null    float64\n",
      " 3   appearances_NHKç´…ç™½æ­Œåˆæˆ¦             947 non-null    float64\n",
      " 4   appearances_ãƒ™ã‚¹ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ            947 non-null    float64\n",
      " 5   appearances_ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ©ã‚¤ãƒ–  947 non-null    float64\n",
      " 6   artist_category                  947 non-null    object \n",
      " 7   type                             947 non-null    object \n",
      " 8   origin                           923 non-null    object \n",
      " 9   spotify_followers                800 non-null    float64\n",
      " 10  spotify_popularity               800 non-null    float64\n",
      " 11  cluster_label                    947 non-null    int64  \n",
      " 12  is_kohaku_performer              947 non-null    int64  \n",
      "dtypes: float64(6), int64(3), object(4)\n",
      "memory usage: 135.9+ KB\n",
      "None\n",
      "\n",
      "--- æ¬ æå€¤ã®ç¢ºèª ---\n",
      "origin                 24\n",
      "spotify_followers     147\n",
      "spotify_popularity    147\n",
      "dtype: int64\n",
      "\n",
      "--- æ¬ æå€¤å‡¦ç†ã¨ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº† ---\n",
      "å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­:\n",
      "    year     artist  total_tv_appearances_yearly  appearances_NHKç´…ç™½æ­Œåˆæˆ¦  \\\n",
      "0   2015        AAA                          6.0                   6.0   \n",
      "3   2015         AI                          0.0                   0.0   \n",
      "15  2015      AKB48                          8.0                   8.0   \n",
      "34  2015  BABYMETAL                          0.0                   0.0   \n",
      "38  2015   BREAKERZ                          0.0                   0.0   \n",
      "\n",
      "    appearances_ãƒ™ã‚¹ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ  appearances_ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ©ã‚¤ãƒ–  spotify_followers  \\\n",
      "0                     0.0                              0.0              756.0   \n",
      "3                     0.0                              0.0            31319.0   \n",
      "15                    0.0                              0.0           511629.0   \n",
      "34                    0.0                              0.0          2597237.0   \n",
      "38                    0.0                              0.0            13246.0   \n",
      "\n",
      "    spotify_popularity  cluster_label  is_kohaku_performer  ...  \\\n",
      "0                 16.0              1                    1  ...   \n",
      "3                 54.0              2                    0  ...   \n",
      "15                59.0              2                    1  ...   \n",
      "34                71.0              0                    0  ...   \n",
      "38                33.0              0                    0  ...   \n",
      "\n",
      "    origin_æ—¥æœ¬, é™å²¡çœŒ, é™å²¡å¸‚, ï¼ˆå¾Œã®, è‘µåŒº, ï¼‰  origin_æ—¥æœ¬, é«˜çŸ¥çœŒ, å—å›½å¸‚  origin_æ—¥æœ¬, é¹¿å…å³¶çœŒ  \\\n",
      "0                             False                False            False   \n",
      "3                             False                False            False   \n",
      "15                            False                False            False   \n",
      "34                            False                False            False   \n",
      "38                            False                False            False   \n",
      "\n",
      "    origin_æ—¥æœ¬, é¹¿å…å³¶çœŒ, é¹¿å…å³¶å¸‚  origin_æ±äº¬éƒ½, åŒ—åŒº  origin_æ±äº¬éƒ½, è¶³ç«‹åŒº, å³¶æ ¹  \\\n",
      "0                   False           False                False   \n",
      "3                   False           False                False   \n",
      "15                  False           False                False   \n",
      "34                  False           False                False   \n",
      "38                  False           False                False   \n",
      "\n",
      "    origin_æ±äº¬éƒ½, ï¼ˆ, å°æ¹¾, è‚²ã¡ï¼‰  origin_éå…¬é–‹  origin_éŸ“å›½  origin_éŸ“å›½, ã‚½ã‚¦ãƒ«ç‰¹åˆ¥å¸‚  \n",
      "0                    False       False      False              False  \n",
      "3                    False       False      False              False  \n",
      "15                   False       False      False              False  \n",
      "34                   False       False      False              False  \n",
      "38                   False       False      False              False  \n",
      "\n",
      "[5 rows x 205 columns]\n",
      "\n",
      "å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æƒ…å ±:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 947 entries, 0 to 7494\n",
      "Columns: 205 entries, year to origin_éŸ“å›½, ã‚½ã‚¦ãƒ«ç‰¹åˆ¥å¸‚\n",
      "dtypes: bool(195), float64(6), int64(3), object(1)\n",
      "memory usage: 294.0+ KB\n",
      "None\n",
      "\n",
      "--- æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ ---\n",
      "\n",
      "=== ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ« ===\n",
      "\n",
      "2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°) è©•ä¾¡:\n",
      "Accuracy: 0.7706\n",
      "Precision: 0.8182\n",
      "Recall: 0.5870\n",
      "F1-Score: 0.6835\n",
      "ROC AUC: 0.7869\n",
      "Confusion Matrix:\n",
      "[[57  6]\n",
      " [19 27]]\n",
      "\n",
      "2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°) çµæœ (äºˆæ¸¬ç¢ºç‡é™é †):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 109 does not match index length 218",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_2024_true, y_pred_2024_log_reg))\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°) çµæœ (äºˆæ¸¬ç¢ºç‡é™é †):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 245\u001b[0m     log_reg_2024_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43martists_2024\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_proba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_proba_2024_log_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_is_performer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_2024_log_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_is_performer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_2024_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mprint\u001b[39m(log_reg_2024_results\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:690\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m    686\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    687\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m         )\n\u001b[0;32m--> 690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: array length 109 does not match index length 218"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Setting warnings to ignore for cleaner output if expected warnings occur (e.g., from pd.get_dummies)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    kouhaku_performers = pd.read_csv('kouhaku_performers_full_2015-2025.csv')\n",
    "    artist_data = pd.read_csv('2artist_data.csv') # Renamed for clarity\n",
    "    artist_master = pd.read_csv('artist_master_categorized_final.csv')\n",
    "    # This file seems to be identical to 2artist_data.csv based on previous error outputs.\n",
    "    # We will prioritize artist_data for features and clarify this to the user.\n",
    "    spotify_popularity_dummy = pd.read_csv('spotify_popularity_cleaned.csv')\n",
    "    analysis_clusters = pd.read_csv('1_analysis_clusters.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - {e}. å…¨ã¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "    exit()\n",
    "\n",
    "print(\"--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº† ---\")\n",
    "\n",
    "# --- Debug: Print columns of analysis_clusters and max year in artist_data ---\n",
    "print(\"\\n--- 1_analysis_clusters.csv ã®åˆ—å ---\")\n",
    "print(analysis_clusters.columns)\n",
    "\n",
    "print(f\"\\n--- artist_data.csv (ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿) ã®æœ€å¤§å¹´: {artist_data['year'].max()} ---\")\n",
    "\n",
    "# è£œè¶³: spotify_popularity_cleaned.csvã®ãƒ‡ãƒ¼ã‚¿ã¯ã€artist_data.csvã®spotifyé–¢é€£åˆ—ã¨é¡ä¼¼ã—ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚\n",
    "# ä»Šå›ã¯ã€artist_data.csvã®spotify_followersã¨spotify_popularityã‚’ä¸»è¦ãªSpotifyæŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "# --- 1. ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---\n",
    "\n",
    "# å¹´ã”ã¨ã®ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã®ç·ãƒ†ãƒ¬ãƒ“å‡ºæ¼”å›æ•°ã‚’é›†è¨ˆ (2artist_dataã‹ã‚‰)\n",
    "artist_tv_appearances = artist_data.groupby(['year', 'artist'])['appearances'].sum().reset_index()\n",
    "artist_tv_appearances.rename(columns={'appearances': 'total_tv_appearances_yearly'}, inplace=True)\n",
    "\n",
    "# ä¸»è¦éŸ³æ¥½ç•ªçµ„ã”ã¨ã®å‡ºæ¼”å›æ•°ã‚’é›†è¨ˆ\n",
    "major_programs = [\n",
    "    'NHKç´…ç™½æ­Œåˆæˆ¦', 'ãƒ™ã‚¹ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ', 'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ©ã‚¤ãƒ–',\n",
    "    'CDTVãƒ©ã‚¤ãƒ–ï¼ãƒ©ã‚¤ãƒ–ï¼', 'FNSæ­Œè¬¡ç¥­', 'Mã‚¹ãƒ† ã‚¦ãƒ«ãƒˆãƒ©SUPER LIVE', 'COUNT DOWN TV', 'ãƒ™ã‚¹ãƒˆãƒ’ãƒƒãƒˆæ­Œè¬¡ç¥­',\n",
    "    'SONGS', 'ãƒã‚ºãƒªã‚ºãƒ 02', 'ã†ãŸã‚³ãƒ³'\n",
    "]\n",
    "\n",
    "# Create pivot table for specific program appearances\n",
    "program_appearances = artist_data.pivot_table(\n",
    "    index=['year', 'artist'],\n",
    "    columns='program',\n",
    "    values='appearances',\n",
    "    aggfunc='sum'\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "# Clean program names for column renaming\n",
    "cleaned_program_cols = {}\n",
    "for col in program_appearances.columns:\n",
    "    if col in ['year', 'artist']:\n",
    "        continue\n",
    "    cleaned_col_name = f'appearances_{col.replace(\" \", \"_\").replace(\"ï¼\", \"\").replace(\"ï¼ˆ\", \"\").replace(\"ï¼‰\", \"\").replace(\"ã€\", \"\").replace(\"ãƒ»\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"ã€œ\", \"\").replace(\"ãƒ»\", \"_\").replace(\"ã€\", \"\").replace(\"ã€Œ\", \"\")}'\n",
    "    cleaned_program_cols[col] = cleaned_col_name\n",
    "\n",
    "program_appearances.rename(columns=cleaned_program_cols, inplace=True)\n",
    "\n",
    "\n",
    "# Merge artist_tv_appearances and program_appearances\n",
    "features_df = pd.merge(artist_tv_appearances, program_appearances, on=['year', 'artist'], how='left')\n",
    "\n",
    "# artist_master_categorized_final.csv ã¨çµåˆ (artist_category, type, origin ã‚’è¿½åŠ )\n",
    "cols_to_merge_from_master = ['artist', 'artist_category', 'type', 'origin']\n",
    "existing_master_cols = [col for col in cols_to_merge_from_master if col in artist_master.columns]\n",
    "if 'Gender' in artist_master.columns:\n",
    "    existing_master_cols.append('Gender')\n",
    "\n",
    "features_df = pd.merge(features_df, artist_master[existing_master_cols], on='artist', how='left')\n",
    "\n",
    "\n",
    "# 2artist_data.csv ã‹ã‚‰ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã”ã¨ã®spotify_followersã¨spotify_popularityï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ã‚’å–å¾—\n",
    "artist_spotify_by_year = artist_data[['year', 'artist', 'spotify_followers', 'spotify_popularity']].dropna(subset=['spotify_followers', 'spotify_popularity']).drop_duplicates(subset=['year', 'artist'], keep='first')\n",
    "\n",
    "# çµåˆ: features_dfã«spotify_followersã¨spotify_popularityï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ã‚’ãƒãƒ¼ã‚¸\n",
    "features_df = pd.merge(features_df, artist_spotify_by_year, on=['year', 'artist'], how='left')\n",
    "\n",
    "\n",
    "# analysis_clusters.csv ã¨çµåˆ (cluster_labelã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ )\n",
    "if 'cluster' in analysis_clusters.columns: # 'cluster_label'ã§ã¯ãªã'cluster'\n",
    "    features_df = pd.merge(features_df, analysis_clusters[['artist', 'cluster']], on='artist', how='left')\n",
    "    features_df.rename(columns={'cluster': 'cluster_label'}, inplace=True) # åˆ—åã‚’'cluster_label'ã«çµ±ä¸€\n",
    "    print(\"info: 'cluster' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚'cluster_label'ã¨ã—ã¦ç‰¹å¾´é‡ã«çµåˆã—ã¾ã™ã€‚\")\n",
    "elif 'cluster_label' in analysis_clusters.columns: # å¿µã®ãŸã‚æ—¢å­˜ã®'cluster_label'ã‚‚ãƒã‚§ãƒƒã‚¯\n",
    "    features_df = pd.merge(features_df, analysis_clusters[['artist', 'cluster_label']], on='artist', how='left')\n",
    "    print(\"info: 'cluster_label' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ç‰¹å¾´é‡ã¨ã—ã¦çµåˆã—ã¾ã™ã€‚\")\n",
    "else:\n",
    "    print(\"warning: 'cluster' åˆ—ã‚‚ 'cluster_label' åˆ—ã‚‚ '1_analysis_clusters.csv' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ç‰¹å¾´é‡ã¯ãƒ¢ãƒ‡ãƒ«ã«ã¯å«ã¾ã‚Œã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "# --- ä¿®æ­£ç‚¹: ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ã‚’å‰Šé™¤ã—ã€(year, artist)ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªçµ„ã¿åˆã‚ã›ã‚’ä¿è¨¼ ---\n",
    "print(\"\\n--- ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ï¼ˆyear, artistï¼‰ã‚’å‰Šé™¤ä¸­ ---\")\n",
    "initial_rows = len(features_df)\n",
    "features_df.drop_duplicates(subset=['year', 'artist'], inplace=True)\n",
    "print(f\"åˆæœŸè¡Œæ•°: {initial_rows}, é‡è¤‡å‰Šé™¤å¾Œè¡Œæ•°: {len(features_df)}\")\n",
    "\n",
    "\n",
    "# --- 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° 'is_kohaku_performer' ã®ä½œæˆ ---\n",
    "\n",
    "# ç´…ç™½å‡ºå ´ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "kohaku_artists_by_year = kouhaku_performers.groupby('year')['artist'].apply(set).to_dict()\n",
    "\n",
    "# features_df ã®å„è¡Œã«ã¤ã„ã¦ã€ç´…ç™½å‡ºå ´è€…ã‹ã©ã†ã‹ã‚’åˆ¤å®š\n",
    "features_df['is_kohaku_performer'] = 0 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯éå‡ºå ´\n",
    "\n",
    "for index, row in features_df.iterrows():\n",
    "    year = row['year']\n",
    "    artist = row['artist']\n",
    "    if year in kohaku_artists_by_year and artist in kohaku_artists_by_year[year]:\n",
    "        features_df.at[index, 'is_kohaku_performer'] = 1\n",
    "\n",
    "print(\"\\n--- ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆå®Œäº† ---\")\n",
    "print(\"is_kohaku_performer ã®åˆ†å¸ƒ:\")\n",
    "print(features_df['is_kohaku_performer'].value_counts())\n",
    "\n",
    "print(\"\\nçµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­:\")\n",
    "print(features_df.head())\n",
    "print(\"\\nçµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æƒ…å ±:\")\n",
    "print(features_df.info())\n",
    "\n",
    "\n",
    "# --- 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨å‰å‡¦ç† ---\n",
    "\n",
    "# æ¬ æå€¤ã®ç¢ºèªã¨å‡¦ç†\n",
    "print(\"\\n--- æ¬ æå€¤ã®ç¢ºèª ---\")\n",
    "print(features_df.isnull().sum()[features_df.isnull().sum() > 0])\n",
    "\n",
    "# æ•°å€¤ç‰¹å¾´é‡ã®æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹ï¼ˆä¾‹: ä¸­å¤®å€¤ã‚„å¹³å‡å€¤ã€ã¾ãŸã¯0ï¼‰\n",
    "numerical_cols = features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'is_kohaku_performer' in numerical_cols:\n",
    "    numerical_cols.remove('is_kohaku_performer')\n",
    "\n",
    "for col in numerical_cols:\n",
    "    features_df[col] = features_df[col].fillna(0) # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®å‡¦ç† (One-Hot Encoding, Label Encoding)\n",
    "categorical_cols = features_df.select_dtypes(include='object').columns.tolist()\n",
    "if 'artist' in categorical_cols:\n",
    "    categorical_cols.remove('artist')\n",
    "\n",
    "# cluster_labelãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿LabelEncoderã‚’é©ç”¨\n",
    "if 'cluster_label' in features_df.columns and features_df['cluster_label'].dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    features_df['cluster_label'] = le.fit_transform(features_df['cluster_label'].fillna('Unknown'))\n",
    "    if 'cluster_label' in categorical_cols:\n",
    "        categorical_cols.remove('cluster_label')\n",
    "elif 'cluster_label' not in features_df.columns:\n",
    "    print(\"warning: 'cluster_label' åˆ—ãŒæœ€çµ‚çš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "\n",
    "features_df = pd.get_dummies(features_df, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "print(\"\\n--- æ¬ æå€¤å‡¦ç†ã¨ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº† ---\")\n",
    "print(\"å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å…ˆé ­:\")\n",
    "print(features_df.head())\n",
    "print(\"\\nå‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æƒ…å ±:\")\n",
    "print(features_df.info())\n",
    "\n",
    "\n",
    "# --- 4. æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã€è©•ä¾¡ã€äºˆæ¸¬ ---\n",
    "\n",
    "# ç›®çš„å¤‰æ•° (y) ã¨ç‰¹å¾´é‡ (X) ã®å®šç¾©\n",
    "X = features_df.drop(['artist', 'is_kohaku_performer'], axis=1) # artistã¯IDãªã®ã§é™¤å¤–\n",
    "y = features_df['is_kohaku_performer']\n",
    "\n",
    "# Xã¨yã®åˆ—ãŒæƒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "if X.empty or X.shape[1] == 0:\n",
    "    print(\"\\nã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ (X) ãŒç©ºã§ã‚ã‚‹ã‹ã€åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿çµ±åˆã¾ãŸã¯ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "    print(\"features_dfã®æœ€çµ‚çš„ãªçŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ (2015-2023) ã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ (2024) ã®åˆ†å‰²\n",
    "X_train_val = X[X['year'] <= 2023]\n",
    "y_train_val = y[X['year'] <= 2023]\n",
    "\n",
    "X_2024 = X[X['year'] == 2024]\n",
    "artists_2024 = features_df[features_df['year'] == 2024]['artist']\n",
    "y_2024_true = y[X['year'] == 2024]\n",
    "\n",
    "# yearåˆ—ã¯å­¦ç¿’ã«å«ã‚ãªã„\n",
    "X_train_val = X_train_val.drop('year', axis=1)\n",
    "if not X_2024.empty:\n",
    "    X_2024_pred = X_2024.drop('year', axis=1)\n",
    "else:\n",
    "    X_2024_pred = pd.DataFrame(columns=X_train_val.columns) # ç©ºã®DataFrameã‚’ä½œæˆ\n",
    "\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "if X_train_val.empty or y_train_val.empty:\n",
    "    print(\"\\nã‚¨ãƒ©ãƒ¼: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚2015-2023å¹´ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ã‹ã€é©åˆ‡ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "    exit()\n",
    "\n",
    "X_train = X_train_val\n",
    "y_train = y_train_val\n",
    "\n",
    "\n",
    "# ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (æ•°å€¤ç‰¹å¾´é‡ã®ã¿)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "\n",
    "print(\"\\n--- æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ ---\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ\n",
    "if not X_train.empty and not y_train.empty:\n",
    "    # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«\n",
    "    print(\"\\n=== ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ« ===\")\n",
    "    log_reg = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', max_iter=1000)\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    if not X_2024_pred.empty:\n",
    "        # åˆ—ã®æ•´åˆæ€§ã‚’ç¢ºä¿\n",
    "        missing_cols_2024 = set(X_train.columns) - set(X_2024_pred.columns)\n",
    "        for c in missing_cols_2024:\n",
    "            X_2024_pred[c] = 0\n",
    "        X_2024_pred = X_2024_pred[X_train.columns] # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜åˆ—é †ã«ä¸¦ã¹æ›¿ãˆ\n",
    "        X_2024_pred_scaled = scaler.transform(X_2024_pred)\n",
    "        X_2024_pred_scaled = pd.DataFrame(X_2024_pred_scaled, columns=X_2024_pred.columns, index=X_2024_pred.index)\n",
    "\n",
    "        # 2024å¹´ã®äºˆæ¸¬ã¨è©•ä¾¡\n",
    "        y_pred_2024_log_reg = log_reg.predict(X_2024_pred_scaled)\n",
    "        y_proba_2024_log_reg = log_reg.predict_proba(X_2024_pred_scaled)[:, 1]\n",
    "\n",
    "        print(\"\\n2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°) è©•ä¾¡:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_2024_true, y_proba_2024_log_reg):.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_2024_true, y_pred_2024_log_reg))\n",
    "\n",
    "        print(\"\\n2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°) çµæœ (äºˆæ¸¬ç¢ºç‡é™é †):\")\n",
    "        log_reg_2024_results = pd.DataFrame({\n",
    "            'artist': artists_2024,\n",
    "            'predicted_proba': y_proba_2024_log_reg,\n",
    "            'predicted_is_performer': y_pred_2024_log_reg,\n",
    "            'true_is_performer': y_2024_true.reset_index(drop=True)\n",
    "        }).sort_values(by='predicted_proba', ascending=False)\n",
    "        print(log_reg_2024_results.head(10))\n",
    "    else:\n",
    "        print(\"2024å¹´ã®äºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®2024å¹´è©•ä¾¡ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "\n",
    "\n",
    "    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«\n",
    "    print(\"\\n=== ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ« ===\")\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    if not X_2024_pred.empty:\n",
    "        # 2024å¹´ã®äºˆæ¸¬ã¨è©•ä¾¡\n",
    "        y_pred_2024_rf = rf_clf.predict(X_2024_pred_scaled)\n",
    "        y_proba_2024_rf = rf_clf.predict_proba(X_2024_pred_scaled)[:, 1]\n",
    "\n",
    "        print(\"\\n2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ) è©•ä¾¡:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_2024_true, y_proba_2024_rf):.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_2024_true, y_pred_2024_rf))\n",
    "\n",
    "        print(\"\\n2024å¹´ ç´…ç™½å‡ºå ´äºˆæ¸¬ (ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ) çµæœ (äºˆæ¸¬ç¢ºç‡é™é †):\")\n",
    "        rf_2024_results = pd.DataFrame({\n",
    "            'artist': artists_2024,\n",
    "            'predicted_proba': y_proba_2024_rf,\n",
    "            'predicted_is_performer': y_pred_2024_rf,\n",
    "            'true_is_performer': y_2024_true.reset_index(drop=True)\n",
    "        }).sort_values(by='predicted_proba', ascending=False)\n",
    "        print(rf_2024_results.head(10))\n",
    "    else:\n",
    "        print(\"2024å¹´ã®äºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®2024å¹´è©•ä¾¡ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "\n",
    "\n",
    "    # 2025å¹´ã®äºˆæ¸¬ã¯ã€ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ\n",
    "    # ã“ã“ã§ã¯2025å¹´ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›ã—ã¦ã‚¹ã‚­ãƒƒãƒ—\n",
    "    print(\"\\n--- 2025å¹´ ç´…ç™½å‡ºå ´ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆäºˆæ¸¬ ---\")\n",
    "    print(f\"è­¦å‘Š: 2025å¹´ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆartist_data.csvã®æœ€å¤§å¹´: {artist_data['year'].max()}ï¼‰ã€‚2025å¹´ã®äºˆæ¸¬ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚\")\n",
    "\n",
    "else:\n",
    "    print(\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨äºˆæ¸¬ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "\n",
    "# --- ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—ã¨ä»Šå¾Œã®èª²é¡Œã®å†ç¢ºèª ---\n",
    "print(\"\\n--- ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—ã¨ä»Šå¾Œã®èª²é¡Œ ---\")\n",
    "print(\"1. **YouTubeãƒ‡ãƒ¼ã‚¿**: ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯YouTubeã®å†ç”Ÿå›æ•°ã‚„ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°ã€ãƒˆãƒ¬ãƒ³ãƒ‰å…¥ã‚Šæƒ…å ±ãªã©ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã‚Œã¯ç´…ç™½é¸è€ƒã®é‡è¦ãªè¦ç´ ã¨è€ƒãˆã‚‰ã‚Œã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’é«˜ã‚ã‚‹ä¸Šã§ä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "print(\"   -> è§£æ±ºç­–: YouTube Data API v3ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦ã§ã™ãŒã€APIã‚­ãƒ¼ã®å–å¾—ã¨ã€éå»ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹ã«å¹´ã®ç™ºè¡¨ç›´å‰ã¾ã§ã®ä¼¸ã³ç‡ï¼‰ã®åé›†ã¯è‡ªå‹•åŒ–ãŒé›£ã—ã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆç¹°ã‚Šè¿”ã—APIã‚³ãƒ¼ãƒ«ã‚’è¡Œã†ã€ã¾ãŸã¯å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’åˆ©ç”¨ï¼‰ã€‚\")\n",
    "print(\"2. **å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆCD/ãƒ‡ã‚¸ã‚¿ãƒ«ï¼‰**: ã‚ªãƒªã‚³ãƒ³ã‚„Billboard Japanãªã©ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã¯ã€äººæ°—ã¨å®Ÿç¸¾ã‚’ç¤ºã™é‡è¦ãªæŒ‡æ¨™ã§ã™ãŒã€ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "print(\"   -> è§£æ±ºç­–: å¤–éƒ¨ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’æ‰‹å‹•ã¾ãŸã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§åé›†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "print(\"3. **ãƒ‡ã‚¸ã‚¿ãƒ«äººæ°—æŒ‡æ¨™ã®æ™‚ç³»åˆ—æ€§**: æä¾›ã•ã‚ŒãŸSpotifyãƒ‡ãƒ¼ã‚¿ã¯ç¾æ™‚ç‚¹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€ç‰¹å®šã®å¹´ã®11æœˆã¾ã§ã®äººæ°—å‹•å‘ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ã„ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "print(\"   -> è§£æ±ºç­–: å„å¹´ã®æœˆæ¬¡ãƒ»é€±æ¬¡ã®äººæ°—åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãªã©ï¼‰ã‚’åˆ¥é€”åé›†ã—ã€ç™ºè¡¨ç›´å‰ã¾ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„ä¼¸ã³ç‡ã‚’ç‰¹å¾´é‡ã¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "print(\"4. **ç´…ç™½é¸è€ƒã®å®šæ€§çš„ãªè¦ç´ **: NHKã®é¸è€ƒã«ã¯ã€Œãã®å¹´ã®é¡”ã€ã€Œè©±é¡Œæ€§ã€ã€Œãƒãƒ©ãƒ³ã‚¹ã€ã¨ã„ã£ãŸå®šæ€§çš„ãªåŸºæº–ã‚‚å«ã¾ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã‚’å®šé‡çš„ãªç‰¹å¾´é‡ã«è½ã¨ã—è¾¼ã‚€ã®ã¯å›°é›£ã§ã™ã€‚\")\n",
    "print(\"   -> è§£æ±ºç­–: ãƒ¢ãƒ‡ãƒ«ã¯å®šé‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®è¦ç´ ã¯å®Œå…¨ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ãŒã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®æ„Ÿæƒ…åˆ†æãªã©é«˜åº¦ãªNLPã‚’ç”¨ã„ã‚‹ã“ã¨ã§ä¸€éƒ¨å–ã‚Šè¾¼ã‚ã‚‹å¯èƒ½æ€§ã¯ã‚ã‚Šã¾ã™ã€‚\")\n",
    "print(\"5. **ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®ç¶²ç¾…æ€§**: ä»Šå›ã®åˆ†æã§ã¯`2artist_data.csv`ã«å«ã¾ã‚Œã‚‹ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã‚’éå‡ºå ´å€™è£œã¨ã—ã¾ã—ãŸã€‚ã“ã‚ŒãŒç´…ç™½é¸è€ƒã§è€ƒæ…®ã•ã‚Œã‚‹ã€å…¨å€™è£œè€…ã€ã‚’ç¶²ç¾…ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ç‚¹ã‚‚ã”ç•™æ„ãã ã•ã„ã€‚ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªå€™è£œè€…ãƒªã‚¹ãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€åˆ¥ã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦ã§ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02fc859-014e-4a6f-9399-6131c18d1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotipy in /opt/conda/lib/python3.11/site-packages (2.25.1)\n",
      "Requirement already satisfied: pytrends in /home/jovyan/.local/lib/python3.11/site-packages (4.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: redis>=3.5.3 in /opt/conda/lib/python3.11/site-packages (from spotipy) (6.2.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.11/site-packages (from spotipy) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from spotipy) (2.2.3)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from pytrends) (6.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spotipy pytrends scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df31164-9391-4c99-9e65-4dd8ef370306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [28:46<00:00, 215.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” 2024å¹´ã®äºˆæ¸¬ã¨æ­£èª¤è©•ä¾¡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [02:20<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Appeared       0.92      0.94      0.93       169\n",
      "    Appeared       0.62      0.55      0.58        29\n",
      "\n",
      "    accuracy                           0.88       198\n",
      "   macro avg       0.77      0.75      0.76       198\n",
      "weighted avg       0.88      0.88      0.88       198\n",
      "\n",
      "\n",
      "ğŸ”® 2025å¹´å‡ºæ¼”äºˆæ¸¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–‰       | 59/198 [00:40<01:31,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from pytrends.request import TrendReq\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# === ğŸ” Spotify èªè¨¼æƒ…å ± ===\n",
    "SPOTIFY_CLIENT_ID = '2892e96c25394749aee2a940363bdbc3'\n",
    "SPOTIFY_CLIENT_SECRET = '4afb990292a24641a522c2adb908b158'\n",
    "\n",
    "# === ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ===\n",
    "df = pd.read_csv(\"kouhaku_performers_full_2015-2025.csv\")\n",
    "df[\"artist\"] = df[\"artist\"].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "df[\"appearances\"] = pd.to_numeric(df[\"appearances\"], errors='coerce').fillna(0)\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "\n",
    "# å¹´ã”ã¨ã®ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆé›†åˆ\n",
    "year_artist = df.groupby(\"year\")[\"artist\"].apply(set).to_dict()\n",
    "all_artists = sorted(set(df[df[\"year\"] < 2024][\"artist\"].dropna().unique()))\n",
    "\n",
    "# === API åˆæœŸåŒ– ===\n",
    "spotify_auth = SpotifyClientCredentials(client_id=SPOTIFY_CLIENT_ID, client_secret=SPOTIFY_CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=spotify_auth)\n",
    "pytrends = TrendReq(hl='ja-JP', tz=540)\n",
    "\n",
    "# === ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¾æ›¸ ===\n",
    "spotify_cache = {}\n",
    "trend_cache = {}\n",
    "\n",
    "# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜é–¢æ•°ï¼ˆå¾Œã‹ã‚‰å†åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹ï¼‰\n",
    "def save_cache():\n",
    "    with open(\"spotify_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(spotify_cache, f)\n",
    "    with open(\"trend_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trend_cache, f)\n",
    "\n",
    "# ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿\n",
    "def load_cache():\n",
    "    global spotify_cache, trend_cache\n",
    "    if os.path.exists(\"spotify_cache.pkl\"):\n",
    "        with open(\"spotify_cache.pkl\", \"rb\") as f:\n",
    "            spotify_cache = pickle.load(f)\n",
    "    if os.path.exists(\"trend_cache.pkl\"):\n",
    "        with open(\"trend_cache.pkl\", \"rb\") as f:\n",
    "            trend_cache = pickle.load(f)\n",
    "\n",
    "load_cache()\n",
    "\n",
    "# === å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿å–å¾— ===\n",
    "def get_spotify_features(artist):\n",
    "    if artist in spotify_cache:\n",
    "        return spotify_cache[artist]\n",
    "    try:\n",
    "        results = sp.search(q=f\"artist:{artist}\", type='artist', limit=1)\n",
    "        items = results['artists']['items']\n",
    "        if items:\n",
    "            popularity = items[0]['popularity']\n",
    "            followers = items[0]['followers']['total']\n",
    "        else:\n",
    "            popularity, followers = 0, 0\n",
    "    except:\n",
    "        popularity, followers = 0, 0\n",
    "    spotify_cache[artist] = (popularity, followers)\n",
    "    return popularity, followers\n",
    "\n",
    "def get_trend_score(artist, year):\n",
    "    key = f\"{artist}_{year}\"\n",
    "    if key in trend_cache:\n",
    "        return trend_cache[key]\n",
    "    try:\n",
    "        pytrends.build_payload([artist], timeframe=f'{year}-01-01 {year}-12-31')\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            score = data[artist].mean()\n",
    "        else:\n",
    "            score = 0\n",
    "    except:\n",
    "        score = 0\n",
    "    trend_cache[key] = score\n",
    "    return score\n",
    "\n",
    "# === å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆ ===\n",
    "features, labels = [], []\n",
    "years_train = range(2016, 2024)\n",
    "print(\"ğŸ§ª å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆä¸­...\")\n",
    "for target_year in tqdm(years_train):\n",
    "    prev_years = list(range(2015, target_year))\n",
    "    current_set = year_artist.get(target_year, set())\n",
    "\n",
    "    for artist in all_artists:\n",
    "        appeared_prev = int(any(artist in year_artist.get(y, set()) for y in prev_years))\n",
    "        recent_appearances = sum(artist in year_artist.get(y, set()) for y in prev_years[-3:])\n",
    "        trend_score = get_trend_score(artist, target_year - 1)\n",
    "        popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "        features.append([\n",
    "            appeared_prev,\n",
    "            recent_appearances,\n",
    "            trend_score,\n",
    "            popularity,\n",
    "            followers\n",
    "        ])\n",
    "        labels.append(int(artist in current_set))\n",
    "        time.sleep(0.1)  # è»½ãåˆ¶é™å›é¿\n",
    "\n",
    "# === ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===\n",
    "X_train = pd.DataFrame(features, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_train = pd.Series(labels)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 2024å¹´ã®æ¤œè¨¼ ===\n",
    "print(\"\\nğŸ” 2024å¹´ã®äºˆæ¸¬ã¨æ­£èª¤è©•ä¾¡...\")\n",
    "X_2024, y_2024, artists_2024 = [], [], []\n",
    "\n",
    "for artist in tqdm(all_artists):\n",
    "    appeared_prev = int(any(artist in year_artist.get(y, set()) for y in range(2015, 2024)))\n",
    "    recent_appearances = sum(artist in year_artist.get(y, set()) for y in range(2021, 2024))\n",
    "    trend_score = get_trend_score(artist, 2023)\n",
    "    popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "    X_2024.append([appeared_prev, recent_appearances, trend_score, popularity, followers])\n",
    "    y_2024.append(int(artist in year_artist.get(2024, set())))\n",
    "    artists_2024.append(artist)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "X_2024_df = pd.DataFrame(X_2024, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_2024_pred = model.predict(X_2024_df)\n",
    "\n",
    "print(classification_report(y_2024, y_2024_pred, target_names=[\"Not Appeared\", \"Appeared\"]))\n",
    "\n",
    "# === 2025å¹´ã®äºˆæ¸¬ ===\n",
    "print(\"\\nğŸ”® 2025å¹´å‡ºæ¼”äºˆæ¸¬...\")\n",
    "X_2025, artists_2025 = [], []\n",
    "\n",
    "for artist in tqdm(all_artists):\n",
    "    appeared_prev = int(any(artist in year_artist.get(y, set()) for y in range(2015, 2025)))\n",
    "    recent_appearances = sum(artist in year_artist.get(y, set()) for y in range(2022, 2025))\n",
    "    trend_score = get_trend_score(artist, 2024)\n",
    "    popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "    X_2025.append([appeared_prev, recent_appearances, trend_score, popularity, followers])\n",
    "    artists_2025.append(artist)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "X_2025_df = pd.DataFrame(X_2025, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_2025_pred = model.predict(X_2025_df)\n",
    "y_2025_prob = model.predict_proba(X_2025_df)[:, 1]  # å‡ºæ¼”ç¢ºç‡\n",
    "\n",
    "# å‡ºæ¼”äºˆæ¸¬ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆä¸€è¦§ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰\n",
    "predicted_df = pd.DataFrame({\n",
    "    \"artist\": artists_2025,\n",
    "    \"predicted\": y_2025_pred,\n",
    "    \"probability\": y_2025_prob\n",
    "})\n",
    "\n",
    "predicted_2025 = predicted_df[predicted_df[\"predicted\"] == 1].sort_values(by=\"probability\", ascending=False)\n",
    "\n",
    "print(\"\\n 2025å¹´ ç´…ç™½å‡ºæ¼”äºˆæ¸¬ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆï¼ˆä¸Šä½å€™è£œï¼‰:\")\n",
    "print(predicted_2025[[\"artist\", \"probability\"]].head(20))\n",
    "\n",
    "# CSVå‡ºåŠ›ï¼ˆä»»æ„ï¼‰\n",
    "predicted_2025.to_csv(\"predicted_kouhaku_2025.csv\", index=False)\n",
    "print(\"\\n å‡ºåŠ›å®Œäº†: predicted_kouhaku_2025.csv\")\n",
    "\n",
    "# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜\n",
    "save_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd85c8-8a07-4136-a8f8-9d444968e50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
