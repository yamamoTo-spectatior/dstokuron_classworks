{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea90604e-1eee-485a-a17e-dfe32e433b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- データ読み込み完了 ---\n",
      "\n",
      "--- 1_analysis_clusters.csv の列名 ---\n",
      "Index(['artist', 'total_tv_appearances', 'spotify_popularity',\n",
      "       'youtube_hit_appearances', 'google_trends_score', 'cluster'],\n",
      "      dtype='object')\n",
      "\n",
      "--- artist_data.csv (特徴量データ) の最大年: 2024 ---\n",
      "info: 'cluster' 列が見つかりました。'cluster_label'として特徴量に結合します。\n",
      "\n",
      "--- データの重複（year, artist）を削除中 ---\n",
      "初期行数: 7498, 重複削除後行数: 947\n",
      "\n",
      "--- データ統合とターゲット変数作成完了 ---\n",
      "is_kohaku_performer の分布:\n",
      "is_kohaku_performer\n",
      "0    546\n",
      "1    401\n",
      "Name: count, dtype: int64\n",
      "\n",
      "統合済みデータフレームの先頭:\n",
      "    year     artist  total_tv_appearances_yearly  appearances_NHK紅白歌合戦  \\\n",
      "0   2015        AAA                          6.0                   6.0   \n",
      "3   2015         AI                          0.0                   0.0   \n",
      "15  2015      AKB48                          8.0                   8.0   \n",
      "34  2015  BABYMETAL                          0.0                   0.0   \n",
      "38  2015   BREAKERZ                          0.0                   0.0   \n",
      "\n",
      "    appearances_ベストアーティスト  appearances_ミュージックステーションスーパーライブ artist_category  \\\n",
      "0                     0.0                              0.0      グループ（性別不明）   \n",
      "3                     0.0                              0.0            女性ソロ   \n",
      "15                    0.0                              0.0          女性グループ   \n",
      "34                    0.0                              0.0          女性グループ   \n",
      "38                    0.0                              0.0      グループ（性別不明）   \n",
      "\n",
      "        type       origin  spotify_followers  spotify_popularity  \\\n",
      "0      Group        Japan              756.0                16.0   \n",
      "3   femaleソロ  Los Angeles            31319.0                54.0   \n",
      "15    女性グループ           日本           511629.0                59.0   \n",
      "34    女性グループ           日本          2597237.0                71.0   \n",
      "38     Group           日本            13246.0                33.0   \n",
      "\n",
      "    cluster_label  is_kohaku_performer  \n",
      "0               1                    1  \n",
      "3               2                    0  \n",
      "15              2                    1  \n",
      "34              0                    0  \n",
      "38              0                    0  \n",
      "\n",
      "統合済みデータフレームの情報:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 947 entries, 0 to 7494\n",
      "Data columns (total 13 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   year                             947 non-null    int64  \n",
      " 1   artist                           947 non-null    object \n",
      " 2   total_tv_appearances_yearly      947 non-null    float64\n",
      " 3   appearances_NHK紅白歌合戦             947 non-null    float64\n",
      " 4   appearances_ベストアーティスト            947 non-null    float64\n",
      " 5   appearances_ミュージックステーションスーパーライブ  947 non-null    float64\n",
      " 6   artist_category                  947 non-null    object \n",
      " 7   type                             947 non-null    object \n",
      " 8   origin                           923 non-null    object \n",
      " 9   spotify_followers                800 non-null    float64\n",
      " 10  spotify_popularity               800 non-null    float64\n",
      " 11  cluster_label                    947 non-null    int64  \n",
      " 12  is_kohaku_performer              947 non-null    int64  \n",
      "dtypes: float64(6), int64(3), object(4)\n",
      "memory usage: 135.9+ KB\n",
      "None\n",
      "\n",
      "--- 欠損値の確認 ---\n",
      "origin                 24\n",
      "spotify_followers     147\n",
      "spotify_popularity    147\n",
      "dtype: int64\n",
      "\n",
      "--- 欠損値処理とカテゴリカル特徴量エンコーディング完了 ---\n",
      "前処理後のデータフレームの先頭:\n",
      "    year     artist  total_tv_appearances_yearly  appearances_NHK紅白歌合戦  \\\n",
      "0   2015        AAA                          6.0                   6.0   \n",
      "3   2015         AI                          0.0                   0.0   \n",
      "15  2015      AKB48                          8.0                   8.0   \n",
      "34  2015  BABYMETAL                          0.0                   0.0   \n",
      "38  2015   BREAKERZ                          0.0                   0.0   \n",
      "\n",
      "    appearances_ベストアーティスト  appearances_ミュージックステーションスーパーライブ  spotify_followers  \\\n",
      "0                     0.0                              0.0              756.0   \n",
      "3                     0.0                              0.0            31319.0   \n",
      "15                    0.0                              0.0           511629.0   \n",
      "34                    0.0                              0.0          2597237.0   \n",
      "38                    0.0                              0.0            13246.0   \n",
      "\n",
      "    spotify_popularity  cluster_label  is_kohaku_performer  ...  \\\n",
      "0                 16.0              1                    1  ...   \n",
      "3                 54.0              2                    0  ...   \n",
      "15                59.0              2                    1  ...   \n",
      "34                71.0              0                    0  ...   \n",
      "38                33.0              0                    0  ...   \n",
      "\n",
      "    origin_日本, 静岡県, 静岡市, （後の, 葵区, ）  origin_日本, 高知県, 南国市  origin_日本, 鹿児島県  \\\n",
      "0                             False                False            False   \n",
      "3                             False                False            False   \n",
      "15                            False                False            False   \n",
      "34                            False                False            False   \n",
      "38                            False                False            False   \n",
      "\n",
      "    origin_日本, 鹿児島県, 鹿児島市  origin_東京都, 北区  origin_東京都, 足立区, 島根  \\\n",
      "0                   False           False                False   \n",
      "3                   False           False                False   \n",
      "15                  False           False                False   \n",
      "34                  False           False                False   \n",
      "38                  False           False                False   \n",
      "\n",
      "    origin_東京都, （, 台湾, 育ち）  origin_非公開  origin_韓国  origin_韓国, ソウル特別市  \n",
      "0                    False       False      False              False  \n",
      "3                    False       False      False              False  \n",
      "15                   False       False      False              False  \n",
      "34                   False       False      False              False  \n",
      "38                   False       False      False              False  \n",
      "\n",
      "[5 rows x 205 columns]\n",
      "\n",
      "前処理後のデータフレームの情報:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 947 entries, 0 to 7494\n",
      "Columns: 205 entries, year to origin_韓国, ソウル特別市\n",
      "dtypes: bool(195), float64(6), int64(3), object(1)\n",
      "memory usage: 294.0+ KB\n",
      "None\n",
      "\n",
      "--- 機械学習モデルの訓練と評価 ---\n",
      "\n",
      "=== ロジスティック回帰モデル ===\n",
      "\n",
      "2024年 紅白出場予測 (ロジスティック回帰) 評価:\n",
      "Accuracy: 0.7706\n",
      "Precision: 0.8182\n",
      "Recall: 0.5870\n",
      "F1-Score: 0.6835\n",
      "ROC AUC: 0.7869\n",
      "Confusion Matrix:\n",
      "[[57  6]\n",
      " [19 27]]\n",
      "\n",
      "2024年 紅白出場予測 (ロジスティック回帰) 結果 (予測確率降順):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 109 does not match index length 218",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_2024_true, y_pred_2024_log_reg))\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2024年 紅白出場予測 (ロジスティック回帰) 結果 (予測確率降順):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 245\u001b[0m     log_reg_2024_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43martists_2024\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_proba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_proba_2024_log_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_is_performer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_2024_log_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_is_performer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_2024_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mprint\u001b[39m(log_reg_2024_results\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:690\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m    686\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    687\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m         )\n\u001b[0;32m--> 690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: array length 109 does not match index length 218"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Setting warnings to ignore for cleaner output if expected warnings occur (e.g., from pd.get_dummies)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    kouhaku_performers = pd.read_csv('kouhaku_performers_full_2015-2025.csv')\n",
    "    artist_data = pd.read_csv('2artist_data.csv') # Renamed for clarity\n",
    "    artist_master = pd.read_csv('artist_master_categorized_final.csv')\n",
    "    # This file seems to be identical to 2artist_data.csv based on previous error outputs.\n",
    "    # We will prioritize artist_data for features and clarify this to the user.\n",
    "    spotify_popularity_dummy = pd.read_csv('spotify_popularity_cleaned.csv')\n",
    "    analysis_clusters = pd.read_csv('1_analysis_clusters.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"エラー: ファイルが見つかりません - {e}. 全てのCSVファイルが正しいディレクトリにあるか確認してください。\")\n",
    "    exit()\n",
    "\n",
    "print(\"--- データ読み込み完了 ---\")\n",
    "\n",
    "# --- Debug: Print columns of analysis_clusters and max year in artist_data ---\n",
    "print(\"\\n--- 1_analysis_clusters.csv の列名 ---\")\n",
    "print(analysis_clusters.columns)\n",
    "\n",
    "print(f\"\\n--- artist_data.csv (特徴量データ) の最大年: {artist_data['year'].max()} ---\")\n",
    "\n",
    "# 補足: spotify_popularity_cleaned.csvのデータは、artist_data.csvのspotify関連列と類似しているようです。\n",
    "# 今回は、artist_data.csvのspotify_followersとspotify_popularityを主要なSpotify指標として使用します。\n",
    "\n",
    "# --- 1. データ統合と特徴量エンジニアリング ---\n",
    "\n",
    "# 年ごとのアーティストの総テレビ出演回数を集計 (2artist_dataから)\n",
    "artist_tv_appearances = artist_data.groupby(['year', 'artist'])['appearances'].sum().reset_index()\n",
    "artist_tv_appearances.rename(columns={'appearances': 'total_tv_appearances_yearly'}, inplace=True)\n",
    "\n",
    "# 主要音楽番組ごとの出演回数を集計\n",
    "major_programs = [\n",
    "    'NHK紅白歌合戦', 'ベストアーティスト', 'ミュージックステーションスーパーライブ',\n",
    "    'CDTVライブ！ライブ！', 'FNS歌謡祭', 'Mステ ウルトラSUPER LIVE', 'COUNT DOWN TV', 'ベストヒット歌謡祭',\n",
    "    'SONGS', 'バズリズム02', 'うたコン'\n",
    "]\n",
    "\n",
    "# Create pivot table for specific program appearances\n",
    "program_appearances = artist_data.pivot_table(\n",
    "    index=['year', 'artist'],\n",
    "    columns='program',\n",
    "    values='appearances',\n",
    "    aggfunc='sum'\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "# Clean program names for column renaming\n",
    "cleaned_program_cols = {}\n",
    "for col in program_appearances.columns:\n",
    "    if col in ['year', 'artist']:\n",
    "        continue\n",
    "    cleaned_col_name = f'appearances_{col.replace(\" \", \"_\").replace(\"！\", \"\").replace(\"（\", \"\").replace(\"）\", \"\").replace(\"、\", \"\").replace(\"・\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"〜\", \"\").replace(\"・\", \"_\").replace(\"」\", \"\").replace(\"「\", \"\")}'\n",
    "    cleaned_program_cols[col] = cleaned_col_name\n",
    "\n",
    "program_appearances.rename(columns=cleaned_program_cols, inplace=True)\n",
    "\n",
    "\n",
    "# Merge artist_tv_appearances and program_appearances\n",
    "features_df = pd.merge(artist_tv_appearances, program_appearances, on=['year', 'artist'], how='left')\n",
    "\n",
    "# artist_master_categorized_final.csv と結合 (artist_category, type, origin を追加)\n",
    "cols_to_merge_from_master = ['artist', 'artist_category', 'type', 'origin']\n",
    "existing_master_cols = [col for col in cols_to_merge_from_master if col in artist_master.columns]\n",
    "if 'Gender' in artist_master.columns:\n",
    "    existing_master_cols.append('Gender')\n",
    "\n",
    "features_df = pd.merge(features_df, artist_master[existing_master_cols], on='artist', how='left')\n",
    "\n",
    "\n",
    "# 2artist_data.csv からアーティストごとのspotify_followersとspotify_popularity（アーティストレベル）を取得\n",
    "artist_spotify_by_year = artist_data[['year', 'artist', 'spotify_followers', 'spotify_popularity']].dropna(subset=['spotify_followers', 'spotify_popularity']).drop_duplicates(subset=['year', 'artist'], keep='first')\n",
    "\n",
    "# 結合: features_dfにspotify_followersとspotify_popularity（アーティストレベル）をマージ\n",
    "features_df = pd.merge(features_df, artist_spotify_by_year, on=['year', 'artist'], how='left')\n",
    "\n",
    "\n",
    "# analysis_clusters.csv と結合 (cluster_labelの存在チェックを追加)\n",
    "if 'cluster' in analysis_clusters.columns: # 'cluster_label'ではなく'cluster'\n",
    "    features_df = pd.merge(features_df, analysis_clusters[['artist', 'cluster']], on='artist', how='left')\n",
    "    features_df.rename(columns={'cluster': 'cluster_label'}, inplace=True) # 列名を'cluster_label'に統一\n",
    "    print(\"info: 'cluster' 列が見つかりました。'cluster_label'として特徴量に結合します。\")\n",
    "elif 'cluster_label' in analysis_clusters.columns: # 念のため既存の'cluster_label'もチェック\n",
    "    features_df = pd.merge(features_df, analysis_clusters[['artist', 'cluster_label']], on='artist', how='left')\n",
    "    print(\"info: 'cluster_label' 列が見つかりました。特徴量として結合します。\")\n",
    "else:\n",
    "    print(\"warning: 'cluster' 列も 'cluster_label' 列も '1_analysis_clusters.csv' に見つかりませんでした。この特徴量はモデルには含まれません。\")\n",
    "\n",
    "# --- 修正点: データの重複を削除し、(year, artist)のユニークな組み合わせを保証 ---\n",
    "print(\"\\n--- データの重複（year, artist）を削除中 ---\")\n",
    "initial_rows = len(features_df)\n",
    "features_df.drop_duplicates(subset=['year', 'artist'], inplace=True)\n",
    "print(f\"初期行数: {initial_rows}, 重複削除後行数: {len(features_df)}\")\n",
    "\n",
    "\n",
    "# --- 2. ターゲット変数 'is_kohaku_performer' の作成 ---\n",
    "\n",
    "# 紅白出場アーティストのセットを作成\n",
    "kohaku_artists_by_year = kouhaku_performers.groupby('year')['artist'].apply(set).to_dict()\n",
    "\n",
    "# features_df の各行について、紅白出場者かどうかを判定\n",
    "features_df['is_kohaku_performer'] = 0 # デフォルトは非出場\n",
    "\n",
    "for index, row in features_df.iterrows():\n",
    "    year = row['year']\n",
    "    artist = row['artist']\n",
    "    if year in kohaku_artists_by_year and artist in kohaku_artists_by_year[year]:\n",
    "        features_df.at[index, 'is_kohaku_performer'] = 1\n",
    "\n",
    "print(\"\\n--- データ統合とターゲット変数作成完了 ---\")\n",
    "print(\"is_kohaku_performer の分布:\")\n",
    "print(features_df['is_kohaku_performer'].value_counts())\n",
    "\n",
    "print(\"\\n統合済みデータフレームの先頭:\")\n",
    "print(features_df.head())\n",
    "print(\"\\n統合済みデータフレームの情報:\")\n",
    "print(features_df.info())\n",
    "\n",
    "\n",
    "# --- 3. 特徴量エンジニアリングと前処理 ---\n",
    "\n",
    "# 欠損値の確認と処理\n",
    "print(\"\\n--- 欠損値の確認 ---\")\n",
    "print(features_df.isnull().sum()[features_df.isnull().sum() > 0])\n",
    "\n",
    "# 数値特徴量の欠損値を埋める（例: 中央値や平均値、または0）\n",
    "numerical_cols = features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'is_kohaku_performer' in numerical_cols:\n",
    "    numerical_cols.remove('is_kohaku_performer')\n",
    "\n",
    "for col in numerical_cols:\n",
    "    features_df[col] = features_df[col].fillna(0) # 欠損値を0で埋める\n",
    "\n",
    "# カテゴリカル特徴量の処理 (One-Hot Encoding, Label Encoding)\n",
    "categorical_cols = features_df.select_dtypes(include='object').columns.tolist()\n",
    "if 'artist' in categorical_cols:\n",
    "    categorical_cols.remove('artist')\n",
    "\n",
    "# cluster_labelが存在する場合のみLabelEncoderを適用\n",
    "if 'cluster_label' in features_df.columns and features_df['cluster_label'].dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    features_df['cluster_label'] = le.fit_transform(features_df['cluster_label'].fillna('Unknown'))\n",
    "    if 'cluster_label' in categorical_cols:\n",
    "        categorical_cols.remove('cluster_label')\n",
    "elif 'cluster_label' not in features_df.columns:\n",
    "    print(\"warning: 'cluster_label' 列が最終的な特徴量データに含まれていません。\")\n",
    "\n",
    "\n",
    "features_df = pd.get_dummies(features_df, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "print(\"\\n--- 欠損値処理とカテゴリカル特徴量エンコーディング完了 ---\")\n",
    "print(\"前処理後のデータフレームの先頭:\")\n",
    "print(features_df.head())\n",
    "print(\"\\n前処理後のデータフレームの情報:\")\n",
    "print(features_df.info())\n",
    "\n",
    "\n",
    "# --- 4. 機械学習モデルの訓練、評価、予測 ---\n",
    "\n",
    "# 目的変数 (y) と特徴量 (X) の定義\n",
    "X = features_df.drop(['artist', 'is_kohaku_performer'], axis=1) # artistはIDなので除外\n",
    "y = features_df['is_kohaku_performer']\n",
    "\n",
    "# Xとyの列が揃っていることを確認\n",
    "if X.empty or X.shape[1] == 0:\n",
    "    print(\"\\nエラー: 特徴量データ (X) が空であるか、列がありません。データ統合または特徴量エンジニアリングに問題がある可能性があります。\")\n",
    "    print(\"features_dfの最終的な状態を確認してください。\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 学習データ (2015-2023) と予測データ (2024) の分割\n",
    "X_train_val = X[X['year'] <= 2023]\n",
    "y_train_val = y[X['year'] <= 2023]\n",
    "\n",
    "X_2024 = X[X['year'] == 2024]\n",
    "artists_2024 = features_df[features_df['year'] == 2024]['artist']\n",
    "y_2024_true = y[X['year'] == 2024]\n",
    "\n",
    "# year列は学習に含めない\n",
    "X_train_val = X_train_val.drop('year', axis=1)\n",
    "if not X_2024.empty:\n",
    "    X_2024_pred = X_2024.drop('year', axis=1)\n",
    "else:\n",
    "    X_2024_pred = pd.DataFrame(columns=X_train_val.columns) # 空のDataFrameを作成\n",
    "\n",
    "\n",
    "# 訓練データが空でないことを確認\n",
    "if X_train_val.empty or y_train_val.empty:\n",
    "    print(\"\\nエラー: 訓練データが空です。2015-2023年のデータが存在しないか、適切に分割されていません。\")\n",
    "    exit()\n",
    "\n",
    "X_train = X_train_val\n",
    "y_train = y_train_val\n",
    "\n",
    "\n",
    "# 特徴量のスケーリング (数値特徴量のみ)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "\n",
    "print(\"\\n--- 機械学習モデルの訓練と評価 ---\")\n",
    "\n",
    "# モデルの訓練と評価は訓練データが存在する場合のみ実行\n",
    "if not X_train.empty and not y_train.empty:\n",
    "    # ロジスティック回帰モデル\n",
    "    print(\"\\n=== ロジスティック回帰モデル ===\")\n",
    "    log_reg = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', max_iter=1000)\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    if not X_2024_pred.empty:\n",
    "        # 列の整合性を確保\n",
    "        missing_cols_2024 = set(X_train.columns) - set(X_2024_pred.columns)\n",
    "        for c in missing_cols_2024:\n",
    "            X_2024_pred[c] = 0\n",
    "        X_2024_pred = X_2024_pred[X_train.columns] # 訓練データと同じ列順に並べ替え\n",
    "        X_2024_pred_scaled = scaler.transform(X_2024_pred)\n",
    "        X_2024_pred_scaled = pd.DataFrame(X_2024_pred_scaled, columns=X_2024_pred.columns, index=X_2024_pred.index)\n",
    "\n",
    "        # 2024年の予測と評価\n",
    "        y_pred_2024_log_reg = log_reg.predict(X_2024_pred_scaled)\n",
    "        y_proba_2024_log_reg = log_reg.predict_proba(X_2024_pred_scaled)[:, 1]\n",
    "\n",
    "        print(\"\\n2024年 紅白出場予測 (ロジスティック回帰) 評価:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_2024_true, y_pred_2024_log_reg):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_2024_true, y_proba_2024_log_reg):.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_2024_true, y_pred_2024_log_reg))\n",
    "\n",
    "        print(\"\\n2024年 紅白出場予測 (ロジスティック回帰) 結果 (予測確率降順):\")\n",
    "        log_reg_2024_results = pd.DataFrame({\n",
    "            'artist': artists_2024,\n",
    "            'predicted_proba': y_proba_2024_log_reg,\n",
    "            'predicted_is_performer': y_pred_2024_log_reg,\n",
    "            'true_is_performer': y_2024_true.reset_index(drop=True)\n",
    "        }).sort_values(by='predicted_proba', ascending=False)\n",
    "        print(log_reg_2024_results.head(10))\n",
    "    else:\n",
    "        print(\"2024年の予測対象データが存在しないため、ロジスティック回帰の2024年評価はスキップされました。\")\n",
    "\n",
    "\n",
    "    # ランダムフォレストモデル\n",
    "    print(\"\\n=== ランダムフォレストモデル ===\")\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    if not X_2024_pred.empty:\n",
    "        # 2024年の予測と評価\n",
    "        y_pred_2024_rf = rf_clf.predict(X_2024_pred_scaled)\n",
    "        y_proba_2024_rf = rf_clf.predict_proba(X_2024_pred_scaled)[:, 1]\n",
    "\n",
    "        print(\"\\n2024年 紅白出場予測 (ランダムフォレスト) 評価:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_2024_true, y_pred_2024_rf):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_2024_true, y_proba_2024_rf):.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_2024_true, y_pred_2024_rf))\n",
    "\n",
    "        print(\"\\n2024年 紅白出場予測 (ランダムフォレスト) 結果 (予測確率降順):\")\n",
    "        rf_2024_results = pd.DataFrame({\n",
    "            'artist': artists_2024,\n",
    "            'predicted_proba': y_proba_2024_rf,\n",
    "            'predicted_is_performer': y_pred_2024_rf,\n",
    "            'true_is_performer': y_2024_true.reset_index(drop=True)\n",
    "        }).sort_values(by='predicted_proba', ascending=False)\n",
    "        print(rf_2024_results.head(10))\n",
    "    else:\n",
    "        print(\"2024年の予測対象データが存在しないため、ランダムフォレストの2024年評価はスキップされました。\")\n",
    "\n",
    "\n",
    "    # 2025年の予測は、データが存在する場合のみ実行\n",
    "    # ここでは2025年の特徴量データが存在しないため、警告メッセージを出力してスキップ\n",
    "    print(\"\\n--- 2025年 紅白出場アーティスト予測 ---\")\n",
    "    print(f\"警告: 2025年の特徴量データがありません（artist_data.csvの最大年: {artist_data['year'].max()}）。2025年の予測はスキップされます。\")\n",
    "\n",
    "else:\n",
    "    print(\"訓練データが空のため、モデルの訓練と予測は実行されませんでした。\")\n",
    "\n",
    "\n",
    "# --- データギャップと今後の課題の再確認 ---\n",
    "print(\"\\n--- データギャップと今後の課題 ---\")\n",
    "print(\"1. **YouTubeデータ**: 現在のデータセットにはYouTubeの再生回数やチャンネル登録者数、トレンド入り情報などが含まれていません。これは紅白選考の重要な要素と考えられるため、モデルの予測精度を高める上で不足しています。\")\n",
    "print(\"   -> 解決策: YouTube Data API v3からのデータ収集が必要ですが、APIキーの取得と、過去の時系列データ（特に年の発表直前までの伸び率）の収集は自動化が難しい場合があります（繰り返しAPIコールを行う、または外部データソースを利用）。\")\n",
    "print(\"2. **売上データ（CD/デジタル）**: オリコンやBillboard Japanなどの売上データは、人気と実績を示す重要な指標ですが、現在のデータセットには含まれていません。\")\n",
    "print(\"   -> 解決策: 外部の売上データを手動またはスクレイピングで収集する必要があります。\")\n",
    "print(\"3. **デジタル人気指標の時系列性**: 提供されたSpotifyデータは現時点のスナップショットである可能性が高く、特定の年の11月までの人気動向を正確に反映しているとは限りません。\")\n",
    "print(\"   -> 解決策: 各年の月次・週次の人気度データ（ストリーム数、ランキングなど）を別途収集し、発表直前までのトレンドや伸び率を特徴量とする必要があります。\")\n",
    "print(\"4. **紅白選考の定性的な要素**: NHKの選考には「その年の顔」「話題性」「バランス」といった定性的な基準も含まれます。これらを定量的な特徴量に落とし込むのは困難です。\")\n",
    "print(\"   -> 解決策: モデルは定量的なデータから学習するため、これらの要素は完全にモデル化することはできませんが、ニュース記事の感情分析など高度なNLPを用いることで一部取り込める可能性はあります。\")\n",
    "print(\"5. **ネガティブサンプルの網羅性**: 今回の分析では`2artist_data.csv`に含まれるアーティストを非出場候補としました。これが紅白選考で考慮される『全候補者』を網羅しているわけではない点もご留意ください。より包括的な候補者リストが必要な場合は、別のデータ収集が必要です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02fc859-014e-4a6f-9399-6131c18d1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotipy in /opt/conda/lib/python3.11/site-packages (2.25.1)\n",
      "Requirement already satisfied: pytrends in /home/jovyan/.local/lib/python3.11/site-packages (4.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: redis>=3.5.3 in /opt/conda/lib/python3.11/site-packages (from spotipy) (6.2.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.11/site-packages (from spotipy) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from spotipy) (2.2.3)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from pytrends) (6.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spotipy pytrends scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df31164-9391-4c99-9e65-4dd8ef370306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 学習データ作成中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [28:46<00:00, 215.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 2024年の予測と正誤評価...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [02:20<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Appeared       0.92      0.94      0.93       169\n",
      "    Appeared       0.62      0.55      0.58        29\n",
      "\n",
      "    accuracy                           0.88       198\n",
      "   macro avg       0.77      0.75      0.76       198\n",
      "weighted avg       0.88      0.88      0.88       198\n",
      "\n",
      "\n",
      "🔮 2025年出演予測...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 59/198 [00:40<01:31,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from pytrends.request import TrendReq\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# === 🔐 Spotify 認証情報 ===\n",
    "SPOTIFY_CLIENT_ID = '2892e96c25394749aee2a940363bdbc3'\n",
    "SPOTIFY_CLIENT_SECRET = '4afb990292a24641a522c2adb908b158'\n",
    "\n",
    "# === 📥 CSVファイル読み込み ===\n",
    "df = pd.read_csv(\"kouhaku_performers_full_2015-2025.csv\")\n",
    "df[\"artist\"] = df[\"artist\"].str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\n",
    "df[\"appearances\"] = pd.to_numeric(df[\"appearances\"], errors='coerce').fillna(0)\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "\n",
    "# 年ごとのアーティスト集合\n",
    "year_artist = df.groupby(\"year\")[\"artist\"].apply(set).to_dict()\n",
    "all_artists = sorted(set(df[df[\"year\"] < 2024][\"artist\"].dropna().unique()))\n",
    "\n",
    "# === API 初期化 ===\n",
    "spotify_auth = SpotifyClientCredentials(client_id=SPOTIFY_CLIENT_ID, client_secret=SPOTIFY_CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=spotify_auth)\n",
    "pytrends = TrendReq(hl='ja-JP', tz=540)\n",
    "\n",
    "# === 🔄 キャッシュ辞書 ===\n",
    "spotify_cache = {}\n",
    "trend_cache = {}\n",
    "\n",
    "# キャッシュ保存関数（後から再利用可能にする）\n",
    "def save_cache():\n",
    "    with open(\"spotify_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(spotify_cache, f)\n",
    "    with open(\"trend_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trend_cache, f)\n",
    "\n",
    "# キャッシュ読み込み\n",
    "def load_cache():\n",
    "    global spotify_cache, trend_cache\n",
    "    if os.path.exists(\"spotify_cache.pkl\"):\n",
    "        with open(\"spotify_cache.pkl\", \"rb\") as f:\n",
    "            spotify_cache = pickle.load(f)\n",
    "    if os.path.exists(\"trend_cache.pkl\"):\n",
    "        with open(\"trend_cache.pkl\", \"rb\") as f:\n",
    "            trend_cache = pickle.load(f)\n",
    "\n",
    "load_cache()\n",
    "\n",
    "# === 外部データ取得 ===\n",
    "def get_spotify_features(artist):\n",
    "    if artist in spotify_cache:\n",
    "        return spotify_cache[artist]\n",
    "    try:\n",
    "        results = sp.search(q=f\"artist:{artist}\", type='artist', limit=1)\n",
    "        items = results['artists']['items']\n",
    "        if items:\n",
    "            popularity = items[0]['popularity']\n",
    "            followers = items[0]['followers']['total']\n",
    "        else:\n",
    "            popularity, followers = 0, 0\n",
    "    except:\n",
    "        popularity, followers = 0, 0\n",
    "    spotify_cache[artist] = (popularity, followers)\n",
    "    return popularity, followers\n",
    "\n",
    "def get_trend_score(artist, year):\n",
    "    key = f\"{artist}_{year}\"\n",
    "    if key in trend_cache:\n",
    "        return trend_cache[key]\n",
    "    try:\n",
    "        pytrends.build_payload([artist], timeframe=f'{year}-01-01 {year}-12-31')\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            score = data[artist].mean()\n",
    "        else:\n",
    "            score = 0\n",
    "    except:\n",
    "        score = 0\n",
    "    trend_cache[key] = score\n",
    "    return score\n",
    "\n",
    "# === 学習データ作成 ===\n",
    "features, labels = [], []\n",
    "years_train = range(2016, 2024)\n",
    "print(\"🧪 学習データ作成中...\")\n",
    "for target_year in tqdm(years_train):\n",
    "    prev_years = list(range(2015, target_year))\n",
    "    current_set = year_artist.get(target_year, set())\n",
    "\n",
    "    for artist in all_artists:\n",
    "        appeared_prev = int(any(artist in year_artist.get(y, set()) for y in prev_years))\n",
    "        recent_appearances = sum(artist in year_artist.get(y, set()) for y in prev_years[-3:])\n",
    "        trend_score = get_trend_score(artist, target_year - 1)\n",
    "        popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "        features.append([\n",
    "            appeared_prev,\n",
    "            recent_appearances,\n",
    "            trend_score,\n",
    "            popularity,\n",
    "            followers\n",
    "        ])\n",
    "        labels.append(int(artist in current_set))\n",
    "        time.sleep(0.1)  # 軽く制限回避\n",
    "\n",
    "# === モデル学習 ===\n",
    "X_train = pd.DataFrame(features, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_train = pd.Series(labels)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 2024年の検証 ===\n",
    "print(\"\\n🔍 2024年の予測と正誤評価...\")\n",
    "X_2024, y_2024, artists_2024 = [], [], []\n",
    "\n",
    "for artist in tqdm(all_artists):\n",
    "    appeared_prev = int(any(artist in year_artist.get(y, set()) for y in range(2015, 2024)))\n",
    "    recent_appearances = sum(artist in year_artist.get(y, set()) for y in range(2021, 2024))\n",
    "    trend_score = get_trend_score(artist, 2023)\n",
    "    popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "    X_2024.append([appeared_prev, recent_appearances, trend_score, popularity, followers])\n",
    "    y_2024.append(int(artist in year_artist.get(2024, set())))\n",
    "    artists_2024.append(artist)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "X_2024_df = pd.DataFrame(X_2024, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_2024_pred = model.predict(X_2024_df)\n",
    "\n",
    "print(classification_report(y_2024, y_2024_pred, target_names=[\"Not Appeared\", \"Appeared\"]))\n",
    "\n",
    "# === 2025年の予測 ===\n",
    "print(\"\\n🔮 2025年出演予測...\")\n",
    "X_2025, artists_2025 = [], []\n",
    "\n",
    "for artist in tqdm(all_artists):\n",
    "    appeared_prev = int(any(artist in year_artist.get(y, set()) for y in range(2015, 2025)))\n",
    "    recent_appearances = sum(artist in year_artist.get(y, set()) for y in range(2022, 2025))\n",
    "    trend_score = get_trend_score(artist, 2024)\n",
    "    popularity, followers = get_spotify_features(artist)\n",
    "\n",
    "    X_2025.append([appeared_prev, recent_appearances, trend_score, popularity, followers])\n",
    "    artists_2025.append(artist)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "X_2025_df = pd.DataFrame(X_2025, columns=[\"appeared_prev\", \"recent_appearances\", \"trend_score\", \"popularity\", \"followers\"])\n",
    "y_2025_pred = model.predict(X_2025_df)\n",
    "y_2025_prob = model.predict_proba(X_2025_df)[:, 1]  # 出演確率\n",
    "\n",
    "# 出演予測アーティスト一覧（スコア付き）\n",
    "predicted_df = pd.DataFrame({\n",
    "    \"artist\": artists_2025,\n",
    "    \"predicted\": y_2025_pred,\n",
    "    \"probability\": y_2025_prob\n",
    "})\n",
    "\n",
    "predicted_2025 = predicted_df[predicted_df[\"predicted\"] == 1].sort_values(by=\"probability\", ascending=False)\n",
    "\n",
    "print(\"\\n 2025年 紅白出演予測アーティスト（上位候補）:\")\n",
    "print(predicted_2025[[\"artist\", \"probability\"]].head(20))\n",
    "\n",
    "# CSV出力（任意）\n",
    "predicted_2025.to_csv(\"predicted_kouhaku_2025.csv\", index=False)\n",
    "print(\"\\n 出力完了: predicted_kouhaku_2025.csv\")\n",
    "\n",
    "# キャッシュ保存\n",
    "save_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd85c8-8a07-4136-a8f8-9d444968e50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
